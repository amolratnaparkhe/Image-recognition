{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from matplotlib.image import imread"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPool2D, Dropout, Flatten, Activation\n",
    "from tensorflow.keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/axr524/Documents/Projects/Image recognition\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"/Users/axr524/Documents/Projects/Image recognition\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['untitled folder',\n",
       " '.DS_Store',\n",
       " 'Image_classifier.ipynb',\n",
       " 'Training',\n",
       " 'Testing',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.listdir(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = data_dir + \"/Training/\"\n",
    "test_path = data_dir + \"/Testing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'.DS_Store' in os.listdir(train_path+'/dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog_img = train_path + '/dogs/' + 'images - 2020-06-05T092954.424.jpg'\n",
    "cat_img = train_path + '/cats/' + 'images - 2020-06-05T094500.934.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = []\n",
    "d2 = []\n",
    "for image_filename in os.listdir(train_path + '/cats/'):\n",
    "    if image_filename == '.DS_Store':\n",
    "        pass\n",
    "    else:\n",
    "        img = imread(train_path + '/cats/' + image_filename)\n",
    "    \n",
    "        dim1,dim2,colors = img.shape\n",
    "        d1.append(dim1)\n",
    "        d2.append(dim2)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<seaborn.axisgrid.JointGrid at 0x1a2d9ed080>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAbIAAAGoCAYAAAAjPmDhAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAas0lEQVR4nO3df6yc1X3n8fc3NhCgaYzBsMQ2MhCLJK0TTC3Ay6pioYRf2XArBQEhjbeLsFbL7oalSmIEKtstKI5SAY3UooVAQ4QhZCkBb0yhFj+2WoSdGnD5EYe1Adc2ptjEmCB+Y777x5zrDNf3+t47c++de2beL2k0z3OeMzPf6wzzyTnPmWciM5EkqVYf63QBkiS1wyCTJFXNIJMkVc0gkyRVzSCTJFVtaqcLGIZLKiX1uuh0AZOdIzJJUtUMMklS1Sb71GJXuH31pkHbv3rCERNciSR1H0dkkqSqGWSSpKoZZJKkqhlkkqSqGWSSpKoZZJKkqrn8fgwNtcxekjR+HJFJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqNrXTBag9t6/eNGj7V084YoIrkaTOMMg6yBCSpPY5tShJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqWtdea9HrGEpSb3BEJkmqWteOyMbTUKM9SdLEc0QmSaqaQSZJqppBJkmqmufIhtFt58NczSmp2xhkxWQKrMlUiyRNdgaZ9soRnKTJziDrUqMd1TkKlFQrF3tIkqrmiEySxohT8Z0RmdnpGoYUEfcDh7T48EOAV8ewnIlk7Z1h7Z1Rc+0w/vW/mplnjOPzV29SB1k7ImJNZi7odB2tsPbOsPbOqLl2qL/+buA5MklS1QwySVLVujnIbux0AW2w9s6w9s6ouXaov/7qde05MklSb+jmEZkkqQcYZJKkqhlkkqSqGWSSpKoZZJKkqk3qIDvjjDMS8ObNm7devo1Yl39mDmlSB9mrr9Z8+TVJmli9+pk5qYNMkqThGGSSpKoZZJKkqhlkkqSqGWSSpKoZZJKkqhlkkqSqGWSSpKoZZJKkqhlkkqSqTe10AZLUra6852nuWL2ZXZlMieCCE2Zzdd+8TpfVdQwySRoHV97zNLet2rR7f1fm7n3DbGw5tShJ4+CO1ZtH1a7WGWSSNA525eC/PDJUu1pnkEnSOJgSMap2tc4gk6RxcMEJs0fVrta52EOSxkH/gg5XLY4/g0ySxsnVffMMrgng1KIkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqwwZZRNwSEdsi4pmmtu9FxC8j4qmI+GlETGs6dnlEbIiI5yLi9Kb2M0rbhohYMvZ/iiSpF41kRPZD4IwBbSuB383MzwP/D7gcICI+B5wP/E55zF9HxJSImAL8FXAm8DnggtJXkqS2DBtkmfkPwI4BbX+fmR+U3VXArLJ9DvDjzHw3M18ENgDHl9uGzHwhM98Dflz6SpLUlrE4R/YfgL8r2zOBzU3HtpS2odr3EBGLI2JNRKzZvn37GJQnSd3Lz8w2gywirgA+AJb1Nw3SLffSvmdj5o2ZuSAzF8yYMaOd8iSp6/mZ2cYvREfEIuBLwKmZ2R9KW4DZTd1mAVvL9lDtkiS1rKURWUScAXwb+HJmvtV0aDlwfkTsFxFHAnOBnwP/CMyNiCMjYl8aC0KWt1e6JEkjGJFFxB3AycAhEbEFuIrGKsX9gJURAbAqM/9jZj4bET8BfkFjyvGSzNxVnuc/Aw8AU4BbMvPZcfh7JEk9Ztggy8wLBmm+eS/9rwGuGaT9PuC+UVUnSdIwvLKHJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWoGmSSpagaZJKlqBpkkqWrDBllE3BIR2yLimaa26RGxMiLWl/uDSntExPcjYkNEPBURxzU9ZlHpvz4iFo3PnyNJ6jUjGZH9EDhjQNsS4MHMnAs8WPYBzgTmltti4AZoBB9wFXACcDxwVX/4SZLUjmGDLDP/AdgxoPkc4NayfSvQ19T+o2xYBUyLiMOB04GVmbkjM18DVrJnOEqSNGqtniM7LDNfBij3h5b2mcDmpn5bSttQ7XuIiMURsSYi1mzfvr3F8iSpN/iZOfaLPWKQttxL+56NmTdm5oLMXDBjxowxLU6Suo2fma0H2StlypByv620bwFmN/WbBWzdS7skSW1pNciWA/0rDxcB9za1f72sXjwReL1MPT4AfDEiDiqLPL5Y2iRJasvU4TpExB3AycAhEbGFxurDpcBPIuIiYBNwbul+H3AWsAF4C/hjgMzcERF/Dvxj6fc/MnPgAhJJkkZt2CDLzAuGOHTqIH0TuGSI57kFuGVU1UmSNAyv7CFJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqWltBFhH/LSKejYhnIuKOiPh4RBwZEasjYn1E3BkR+5a++5X9DeX4nLH4AyRJva3lIIuImcB/BRZk5u8CU4Dzge8C12XmXOA14KLykIuA1zLz08B1pZ8kSW1pd2pxKrB/REwFDgBeBk4B7irHbwX6yvY5ZZ9y/NSIiDZfX5LU41oOssx8CfgLYBONAHsdeBzYmZkflG5bgJlleyawuTz2g9L/4IHPGxGLI2JNRKzZvn17q+VJUk/wM7O9qcWDaIyyjgQ+BRwInDlI1+x/yF6O/aYh88bMXJCZC2bMmNFqeZLUE/zMbG9q8Q+AFzNze2a+D9wN/GtgWplqBJgFbC3bW4DZAOX4J4Edbby+JEltBdkm4MSIOKCc6zoV+AXwMPCV0mcRcG/ZXl72Kccfysw9RmSSJI1GO+fIVtNYtPEE8HR5rhuBbwOXRcQGGufAbi4PuRk4uLRfBixpo25JkoDGqsOWZeZVwFUDml8Ajh+k7zvAue28niRJA3llD0lS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLVDDJJUtUMMklS1QwySVLV2gqyiJgWEXdFxC8jYl1ELIyI6RGxMiLWl/uDSt+IiO9HxIaIeCoijhubP0GS1MvaHZH9JXB/Zn4G+AKwDlgCPJiZc4EHyz7AmcDcclsM3NDma0uS1HqQRcRvA78P3AyQme9l5k7gHODW0u1WoK9snwP8KBtWAdMi4vCWK5ckifZGZEcB24G/iYgnI+IHEXEgcFhmvgxQ7g8t/WcCm5sev6W0fURELI6INRGxZvv27W2UJ0ndz8/M9oJsKnAccENmzgfe5DfTiIOJQdpyj4bMGzNzQWYumDFjRhvlSVL38zOzvSDbAmzJzNVl/y4awfZK/5Rhud/W1H920+NnAVvbeH1JkloPssz8F2BzRBxTmk4FfgEsBxaVtkXAvWV7OfD1snrxROD1/ilISZJaNbXNx/8XYFlE7Au8APwxjXD8SURcBGwCzi197wPOAjYAb5W+kiS1pa0gy8y1wIJBDp06SN8ELmnn9SRJGsgre0iSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqNrXdJ4iIKcAa4KXM/FJEHAn8GJgOPAH8UWa+FxH7AT8Cfg/4FXBeZm5s9/XbdeFNj/Ho8zt275909HSWXbywgxVJkkZjLEZk3wDWNe1/F7guM+cCrwEXlfaLgNcy89PAdaVfRw0MMYBHn9/BhTc91qGKJEmj1VaQRcQs4GzgB2U/gFOAu0qXW4G+sn1O2accP7X075iBITZcuyRNZjvefK/TJXREuyOy64FvAR+W/YOBnZn5QdnfAsws2zOBzQDl+OulvyRJLWs5yCLiS8C2zHy8uXmQrjmCY83Puzgi1kTEmu3bt7daniT1hObPzDd29uZsUjsjspOAL0fERhqLO06hMUKbFhH9i0hmAVvL9hZgNkA5/klgj3/1zLwxMxdk5oIZM2a0Ud4I/oCjp4+qXZImm+bPzE9M683PrpaDLDMvz8xZmTkHOB94KDMvBB4GvlK6LQLuLdvLyz7l+EOZuceIbCItu3jhHqH18SnBo8/vYM6SFcxZsoLTrn2kM8VJkkZkPL5H9m3gsojYQOMc2M2l/Wbg4NJ+GbBkHF571JZdvJCNS89m49KzmXvogbyz66PZun7bm5xwzcoOVSdJGk7b3yMDyMxHgEfK9gvA8YP0eQc4dyxeb7ys3/bmoO2vvPEe9zz5En3zZw56XJImi9tXbwLgqycc0eFKJo5X9hihK376dKdLkCQNwiAboTff29XpEiRJgzDImkz9WEe/ny1JaoFB1uQvzv3CXo8fffl9XHmPU4ySNJkYZE2GW8yxK5PbVm0yzCRpEjHIBvjaicOv9Llt1aYJqESSWnf76k27VzB2uzFZft9Nru6bB8Adqzezay/f156zZAUBvLj07AmqTJI0GEdkg7i6bx7Pf+esYfslcOSSFeNfkCRpSAbZXhy475Rh+3T0GluSJINsb675w3kj6nfPky+NcyWSpKEYZHvRN38mcw89cNh+/335sxNQjSRpMAbZMFZedvKwP+uy8+33mbNkhcvyJU06/asXu3kVo0E2AssuXsj15x07bL/bVm1ijos/JGlCGWQj1Dd/5oi+YwYYZpI0gQyyUbi6bx4HHbDPiPoaZpI0MQyyUbrq3/3OiPsaZpI0/gyyUeqbP5PDPrHviPv769KSNL68RFULVl9xGidcs5JX3nhv2L6vvPEeF970GMsuXjgBlUnS3k2WlYtj+QvWjshatPqK04Zdlt/v0ed3uDRfksaJQdaG0Yyyblu1ic9fdf84ViNJvckga9PGUVz9/tfv7uLTl7sARJLGkkE2BjYuPXtEl7IC+CDhwpseG+eKJKl3GGRjZOVlJ4+476PP7+AzV9w3fsVIUg9x1eIY2rj07BF/d+ydXcmcJSs46ejprmiU1HNGsnpypCsbHZGNsdGcMwNHZ5LULoNsHGxcejZTY+T939mVhpkktcggGycbvjO6kdk7u9Ll+ZLUAoNsHI3kp1+a/frdXcxZsoLTrn1kfAqSpC7kYo9x9L0Hnmvpceu3vcmRS1bw4ijPt0nSeBnLS0qNNUdk42jrzrdbfmwCR3r1fEkalkE2jj41bf+2Hp80fgrGhSCSNDSDbBx98/Rj2H+fKR9p23+fKSO+Cki//u+c3fPkS2NZniR1Bc+RjaO++TOBxrmyrTvf5lPT9uebpx+zu320P7x56Z1rWfPPO7i6b96Y1ypJtTLIxlnf/Jm7g2ug0VwJpN9tqzZx26pNfCwaJ18NNUm9ziDrsP4rgRy5ZAU5isd9mI1QW/3Cr0Z1nUdJasXAS0pNplWMniObJF4cxRX0m63f9iZzlqzwhzsl9SyDbBJZednJo75WY7/bVm1yQYiknmSQTUIbl57NSUdPb+mxl965lmOu/DsDTVLPMMgmqWUXL2Tj0rP52omjn4d+94MPufTOtf6Ap6SeYJBNclf3zeP6845lFBfT3+3R53d4/kxS13PVYgX6l/Cfdu0jrN/25qgf379kPwIudMm+1FMm0+rC8eKIrCLtLAYByLJk3ylHSd2k5SCLiNkR8XBErIuIZyPiG6V9ekSsjIj15f6g0h4R8f2I2BART0XEcWP1R/Sa0f5w50D9U479N4NNUs3amVr8APiTzHwiIj4BPB4RK4F/DzyYmUsjYgmwBPg2cCYwt9xOAG4o92pB8w93XnjTYzz6/I6Wn6s/2IC2RnyS1Aktj8gy8+XMfKJsvwGsA2YC5wC3lm63An1l+xzgR9mwCpgWEYe3XLl2W3bxQq4/71j236f9mWJHaJJqMyaLPSJiDjAfWA0clpkvQyPsIuLQ0m0msLnpYVtK28sDnmsxsBjgiCO6/yTlWGm+puNYjtAO3HcK1/zhvCGvFymps5o/Mw/5V73532lkjuYKf4M8QcRvAf8HuCYz746InZk5ren4a5l5UESsAL6Tmf+3tD8IfCszHx/quRcsWJBr1qxpq75edc+TL/Fn//tZXnvr/TF7zuvPO9ZAkybeiM+IH/XZz+fVP/zZsP0qXck45L9DWyOyiNgH+FtgWWbeXZpfiYjDy2jscGBbad8CzG56+Cxgazuvr6E1j9DGKtQuvXMtl/1kLR8mzBzwkzSS1CntrFoM4GZgXWZe23RoObCobC8C7m1q/3pZvXgi8Hr/FKTGV9/8mTz5p18ck/NoH5YB/Es73+byu5/2UliSOq6dT7WTgD8CTomIteV2FrAUOC0i1gOnlX2A+4AXgA3ATcB/auO11YK++TNZ9+dn8rUTj2BKtLF+v3j7/V1874HnxqAySWpdy1OL5VzXUJ+Gpw7SP4FLWn09jZ2r++Z95Ooe7SwO2brz7bEqS5Ja4iWqxLKLF+7eHu1lsD41bf/xKEmSRswg00esvOxkrrznaW5btWnYvvvvM4Vvnn7MBFQlaSSmH7hvrSsS22KQaQ8Dpx7vefIlvvfAc7y0822mRLAr01WLkiYNg0zDal7KL0mTjVe/lyRVzSCTJFXNIJMkVc0gkyRVzSCTJFXNIJMkVc0gkyRVzSCTJFXNIJMkVc0re0hSl9jx5nvcvnr466R22/UYHZFJkqpmkEmSqtZ1U4tzlqzYo23j0rM7UIkkaSJ01YhssBDbW7skqX5dFWSSpN7TdVOLktSrevUXoh2RSZKqZpBJkqrWVUE21OpEVy1KUvfqunNkhpYk9ZauGpFJknqPQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSaqaQSZJqppBJkmqmkEmSapaZGanaxhSRGwH/rnFhx8CvDqG5Uwka+8Ma++MmmuH8a//1cw8YyQdI+L+kfbtJpM6yNoREWsyc0Gn62iFtXeGtXdGzbVD/fV3A6cWJUlVM8gkSVXr5iC7sdMFtMHaO8PaO6Pm2qH++qvXtefIJEm9oZtHZJKkHmCQSZKqVm2QRcQtEbEtIp5papseESsjYn25P6i0R0R8PyI2RMRTEXFcB+ueHREPR8S6iHg2Ir5RS+2lno9HxM8j4p9K/X9W2o+MiNWl/jsjYt/Svl/Z31COz+lw/VMi4smI+FlNdZeaNkbE0xGxNiLWlLZa3jfTIuKuiPhlee8vrKH2iDim/Hv3334dEZfWUHsvqTbIgB8CA7/4twR4MDPnAg+WfYAzgbnlthi4YYJqHMwHwJ9k5meBE4FLIuJz1FE7wLvAKZn5BeBY4IyIOBH4LnBdqf814KLS/yLgtcz8NHBd6ddJ3wDWNe3XUne/f5uZxzZ9b6mW981fAvdn5meAL9D432DS156Zz5V/72OB3wPeAn5KBbX3lMys9gbMAZ5p2n8OOLxsHw48V7b/J3DBYP06fQPuBU6rtPYDgCeAE2hc2WBqaV8IPFC2HwAWlu2ppV90qN5ZND50TgF+BkQNdTfVvxE4ZEDbpH/fAL8NvDjw36+G2gfU+0Xg0Rpr7/ZbzSOywRyWmS8DlPtDS/tMYHNTvy2lraPKdNV8YDUV1V6m59YC24CVwPPAzsz8oHRprnF3/eX468DBE1vxbtcD3wI+LPsHU0fd/RL4+4h4PCIWl7Ya3jdHAduBvynTuj+IiAOpo/Zm5wN3lO3aau9q3RZkQ4lB2jr6vYOI+C3gb4FLM/PXe+s6SFtHa8/MXdmYapkFHA98drBu5X5S1B8RXwK2Zebjzc2DdJ1UdQ9wUmYeR2P66pKI+P299J1M9U8FjgNuyMz5wJv8ZipuMJOpdgDKudMvA/9ruK6DtHX6fdP1ui3IXomIwwHK/bbSvgWY3dRvFrB1gmvbLSL2oRFiyzLz7tJcRe3NMnMn8AiNc33TImJqOdRc4+76y/FPAjsmtlIATgK+HBEbgR/TmF68nslf926ZubXcb6NxnuZ46njfbAG2ZObqsn8XjWCrofZ+ZwJPZOYrZb+m2rtetwXZcmBR2V5E4/xTf/vXy4qiE4HX+6cFJlpEBHAzsC4zr206NOlrB4iIGRExrWzvD/wBjRP3DwNfKd0G1t//d30FeCjLyYOJlJmXZ+aszJxDY4roocy8kEled7+IODAiPtG/TeN8zTNU8L7JzH8BNkfEMaXpVOAXVFB7kwv4zbQi1FV79+v0SbpWbzTeVC8D79P4f0EX0TiH8SCwvtxPL30D+Csa53KeBhZ0sO5/Q2Oq4SlgbbmdVUPtpZ7PA0+W+p8B/rS0HwX8HNhAY/plv9L+8bK/oRw/ahK8d04GflZT3aXOfyq3Z4ErSnst75tjgTXlfXMPcFBFtR8A/Ar4ZFNbFbX3ys1LVEmSqtZtU4uSpB5jkEmSqmaQSZKqZpBJkqpmkEmSqmaQSZKqZpBJkqr2/wEGt1t64cviygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x432 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.jointplot(d1,d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "186.54090150250417"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "265.36060100166947"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(d2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_shape = (150,200,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(d1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(d2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data augmentation: image transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ImageDataGenerator in module tensorflow.python.keras.preprocessing.image:\n",
      "\n",
      "class ImageDataGenerator(keras_preprocessing.image.image_data_generator.ImageDataGenerator)\n",
      " |  ImageDataGenerator(featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)\n",
      " |  \n",
      " |  Generate batches of tensor image data with real-time data augmentation.\n",
      " |  \n",
      " |   The data will be looped over (in batches).\n",
      " |  \n",
      " |  Arguments:\n",
      " |      featurewise_center: Boolean.\n",
      " |          Set input mean to 0 over the dataset, feature-wise.\n",
      " |      samplewise_center: Boolean. Set each sample mean to 0.\n",
      " |      featurewise_std_normalization: Boolean.\n",
      " |          Divide inputs by std of the dataset, feature-wise.\n",
      " |      samplewise_std_normalization: Boolean. Divide each input by its std.\n",
      " |      zca_epsilon: epsilon for ZCA whitening. Default is 1e-6.\n",
      " |      zca_whitening: Boolean. Apply ZCA whitening.\n",
      " |      rotation_range: Int. Degree range for random rotations.\n",
      " |      width_shift_range: Float, 1-D array-like or int\n",
      " |          - float: fraction of total width, if < 1, or pixels if >= 1.\n",
      " |          - 1-D array-like: random elements from the array.\n",
      " |          - int: integer number of pixels from interval\n",
      " |              `(-width_shift_range, +width_shift_range)`\n",
      " |          - With `width_shift_range=2` possible values\n",
      " |              are integers `[-1, 0, +1]`,\n",
      " |              same as with `width_shift_range=[-1, 0, +1]`,\n",
      " |              while with `width_shift_range=1.0` possible values are floats\n",
      " |              in the interval [-1.0, +1.0).\n",
      " |      height_shift_range: Float, 1-D array-like or int\n",
      " |          - float: fraction of total height, if < 1, or pixels if >= 1.\n",
      " |          - 1-D array-like: random elements from the array.\n",
      " |          - int: integer number of pixels from interval\n",
      " |              `(-height_shift_range, +height_shift_range)`\n",
      " |          - With `height_shift_range=2` possible values\n",
      " |              are integers `[-1, 0, +1]`,\n",
      " |              same as with `height_shift_range=[-1, 0, +1]`,\n",
      " |              while with `height_shift_range=1.0` possible values are floats\n",
      " |              in the interval [-1.0, +1.0).\n",
      " |      brightness_range: Tuple or list of two floats. Range for picking\n",
      " |          a brightness shift value from.\n",
      " |      shear_range: Float. Shear Intensity\n",
      " |          (Shear angle in counter-clockwise direction in degrees)\n",
      " |      zoom_range: Float or [lower, upper]. Range for random zoom.\n",
      " |          If a float, `[lower, upper] = [1-zoom_range, 1+zoom_range]`.\n",
      " |      channel_shift_range: Float. Range for random channel shifts.\n",
      " |      fill_mode: One of {\"constant\", \"nearest\", \"reflect\" or \"wrap\"}.\n",
      " |          Default is 'nearest'.\n",
      " |          Points outside the boundaries of the input are filled\n",
      " |          according to the given mode:\n",
      " |          - 'constant': kkkkkkkk|abcd|kkkkkkkk (cval=k)\n",
      " |          - 'nearest':  aaaaaaaa|abcd|dddddddd\n",
      " |          - 'reflect':  abcddcba|abcd|dcbaabcd\n",
      " |          - 'wrap':  abcdabcd|abcd|abcdabcd\n",
      " |      cval: Float or Int.\n",
      " |          Value used for points outside the boundaries\n",
      " |          when `fill_mode = \"constant\"`.\n",
      " |      horizontal_flip: Boolean. Randomly flip inputs horizontally.\n",
      " |      vertical_flip: Boolean. Randomly flip inputs vertically.\n",
      " |      rescale: rescaling factor. Defaults to None.\n",
      " |          If None or 0, no rescaling is applied,\n",
      " |          otherwise we multiply the data by the value provided\n",
      " |          (after applying all other transformations).\n",
      " |      preprocessing_function: function that will be implied on each input.\n",
      " |          The function will run after the image is resized and augmented.\n",
      " |          The function should take one argument:\n",
      " |          one image (Numpy tensor with rank 3),\n",
      " |          and should output a Numpy tensor with the same shape.\n",
      " |      data_format: Image data format,\n",
      " |          either \"channels_first\" or \"channels_last\".\n",
      " |          \"channels_last\" mode means that the images should have shape\n",
      " |          `(samples, height, width, channels)`,\n",
      " |          \"channels_first\" mode means that the images should have shape\n",
      " |          `(samples, channels, height, width)`.\n",
      " |          It defaults to the `image_data_format` value found in your\n",
      " |          Keras config file at `~/.keras/keras.json`.\n",
      " |          If you never set it, then it will be \"channels_last\".\n",
      " |      validation_split: Float. Fraction of images reserved for validation\n",
      " |          (strictly between 0 and 1).\n",
      " |      dtype: Dtype to use for the generated arrays.\n",
      " |  \n",
      " |  Examples:\n",
      " |  \n",
      " |  Example of using `.flow(x, y)`:\n",
      " |  \n",
      " |  ```python\n",
      " |  (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
      " |  y_train = np_utils.to_categorical(y_train, num_classes)\n",
      " |  y_test = np_utils.to_categorical(y_test, num_classes)\n",
      " |  datagen = ImageDataGenerator(\n",
      " |      featurewise_center=True,\n",
      " |      featurewise_std_normalization=True,\n",
      " |      rotation_range=20,\n",
      " |      width_shift_range=0.2,\n",
      " |      height_shift_range=0.2,\n",
      " |      horizontal_flip=True)\n",
      " |  # compute quantities required for featurewise normalization\n",
      " |  # (std, mean, and principal components if ZCA whitening is applied)\n",
      " |  datagen.fit(x_train)\n",
      " |  # fits the model on batches with real-time data augmentation:\n",
      " |  model.fit_generator(datagen.flow(x_train, y_train, batch_size=32),\n",
      " |                      steps_per_epoch=len(x_train) / 32, epochs=epochs)\n",
      " |  # here's a more \"manual\" example\n",
      " |  for e in range(epochs):\n",
      " |      print('Epoch', e)\n",
      " |      batches = 0\n",
      " |      for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):\n",
      " |          model.fit(x_batch, y_batch)\n",
      " |          batches += 1\n",
      " |          if batches >= len(x_train) / 32:\n",
      " |              # we need to break the loop by hand because\n",
      " |              # the generator loops indefinitely\n",
      " |              break\n",
      " |  ```\n",
      " |  \n",
      " |  Example of using `.flow_from_directory(directory)`:\n",
      " |  \n",
      " |  ```python\n",
      " |  train_datagen = ImageDataGenerator(\n",
      " |          rescale=1./255,\n",
      " |          shear_range=0.2,\n",
      " |          zoom_range=0.2,\n",
      " |          horizontal_flip=True)\n",
      " |  test_datagen = ImageDataGenerator(rescale=1./255)\n",
      " |  train_generator = train_datagen.flow_from_directory(\n",
      " |          'data/train',\n",
      " |          target_size=(150, 150),\n",
      " |          batch_size=32,\n",
      " |          class_mode='binary')\n",
      " |  validation_generator = test_datagen.flow_from_directory(\n",
      " |          'data/validation',\n",
      " |          target_size=(150, 150),\n",
      " |          batch_size=32,\n",
      " |          class_mode='binary')\n",
      " |  model.fit_generator(\n",
      " |          train_generator,\n",
      " |          steps_per_epoch=2000,\n",
      " |          epochs=50,\n",
      " |          validation_data=validation_generator,\n",
      " |          validation_steps=800)\n",
      " |  ```\n",
      " |  \n",
      " |  Example of transforming images and masks together.\n",
      " |  \n",
      " |  ```python\n",
      " |  # we create two instances with the same arguments\n",
      " |  data_gen_args = dict(featurewise_center=True,\n",
      " |                       featurewise_std_normalization=True,\n",
      " |                       rotation_range=90,\n",
      " |                       width_shift_range=0.1,\n",
      " |                       height_shift_range=0.1,\n",
      " |                       zoom_range=0.2)\n",
      " |  image_datagen = ImageDataGenerator(**data_gen_args)\n",
      " |  mask_datagen = ImageDataGenerator(**data_gen_args)\n",
      " |  # Provide the same seed and keyword arguments to the fit and flow methods\n",
      " |  seed = 1\n",
      " |  image_datagen.fit(images, augment=True, seed=seed)\n",
      " |  mask_datagen.fit(masks, augment=True, seed=seed)\n",
      " |  image_generator = image_datagen.flow_from_directory(\n",
      " |      'data/images',\n",
      " |      class_mode=None,\n",
      " |      seed=seed)\n",
      " |  mask_generator = mask_datagen.flow_from_directory(\n",
      " |      'data/masks',\n",
      " |      class_mode=None,\n",
      " |      seed=seed)\n",
      " |  # combine generators into one which yields image and masks\n",
      " |  train_generator = zip(image_generator, mask_generator)\n",
      " |  model.fit_generator(\n",
      " |      train_generator,\n",
      " |      steps_per_epoch=2000,\n",
      " |      epochs=50)\n",
      " |  ```\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      ImageDataGenerator\n",
      " |      keras_preprocessing.image.image_data_generator.ImageDataGenerator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, featurewise_center=False, samplewise_center=False, featurewise_std_normalization=False, samplewise_std_normalization=False, zca_whitening=False, zca_epsilon=1e-06, rotation_range=0, width_shift_range=0.0, height_shift_range=0.0, brightness_range=None, shear_range=0.0, zoom_range=0.0, channel_shift_range=0.0, fill_mode='nearest', cval=0.0, horizontal_flip=False, vertical_flip=False, rescale=None, preprocessing_function=None, data_format=None, validation_split=0.0, dtype=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from keras_preprocessing.image.image_data_generator.ImageDataGenerator:\n",
      " |  \n",
      " |  apply_transform(self, x, transform_parameters)\n",
      " |      Applies a transformation to an image according to given parameters.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: 3D tensor, single image.\n",
      " |          transform_parameters: Dictionary with string - parameter pairs\n",
      " |              describing the transformation.\n",
      " |              Currently, the following parameters\n",
      " |              from the dictionary are used:\n",
      " |              - `'theta'`: Float. Rotation angle in degrees.\n",
      " |              - `'tx'`: Float. Shift in the x direction.\n",
      " |              - `'ty'`: Float. Shift in the y direction.\n",
      " |              - `'shear'`: Float. Shear angle in degrees.\n",
      " |              - `'zx'`: Float. Zoom in the x direction.\n",
      " |              - `'zy'`: Float. Zoom in the y direction.\n",
      " |              - `'flip_horizontal'`: Boolean. Horizontal flip.\n",
      " |              - `'flip_vertical'`: Boolean. Vertical flip.\n",
      " |              - `'channel_shift_intencity'`: Float. Channel shift intensity.\n",
      " |              - `'brightness'`: Float. Brightness shift intensity.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A transformed version of the input (same shape).\n",
      " |  \n",
      " |  fit(self, x, augment=False, rounds=1, seed=None)\n",
      " |      Fits the data generator to some sample data.\n",
      " |      \n",
      " |      This computes the internal data stats related to the\n",
      " |      data-dependent transformations, based on an array of sample data.\n",
      " |      \n",
      " |      Only required if `featurewise_center` or\n",
      " |      `featurewise_std_normalization` or `zca_whitening` are set to True.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Sample data. Should have rank 4.\n",
      " |           In case of grayscale data,\n",
      " |           the channels axis should have value 1, in case\n",
      " |           of RGB data, it should have value 3, and in case\n",
      " |           of RGBA data, it should have value 4.\n",
      " |          augment: Boolean (default: False).\n",
      " |              Whether to fit on randomly augmented samples.\n",
      " |          rounds: Int (default: 1).\n",
      " |              If using data augmentation (`augment=True`),\n",
      " |              this is how many augmentation passes over the data to use.\n",
      " |          seed: Int (default: None). Random seed.\n",
      " |  \n",
      " |  flow(self, x, y=None, batch_size=32, shuffle=True, sample_weight=None, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None)\n",
      " |      Takes data & label arrays, generates batches of augmented data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Input data. Numpy array of rank 4 or a tuple.\n",
      " |              If tuple, the first element\n",
      " |              should contain the images and the second element\n",
      " |              another numpy array or a list of numpy arrays\n",
      " |              that gets passed to the output\n",
      " |              without any modifications.\n",
      " |              Can be used to feed the model miscellaneous data\n",
      " |              along with the images.\n",
      " |              In case of grayscale data, the channels axis of the image array\n",
      " |              should have value 1, in case\n",
      " |              of RGB data, it should have value 3, and in case\n",
      " |              of RGBA data, it should have value 4.\n",
      " |          y: Labels.\n",
      " |          batch_size: Int (default: 32).\n",
      " |          shuffle: Boolean (default: True).\n",
      " |          sample_weight: Sample weights.\n",
      " |          seed: Int (default: None).\n",
      " |          save_to_dir: None or str (default: None).\n",
      " |              This allows you to optionally specify a directory\n",
      " |              to which to save the augmented pictures being generated\n",
      " |              (useful for visualizing what you are doing).\n",
      " |          save_prefix: Str (default: `''`).\n",
      " |              Prefix to use for filenames of saved pictures\n",
      " |              (only relevant if `save_to_dir` is set).\n",
      " |          save_format: one of \"png\", \"jpeg\"\n",
      " |              (only relevant if `save_to_dir` is set). Default: \"png\".\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |              `validation_split` is set in `ImageDataGenerator`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          An `Iterator` yielding tuples of `(x, y)`\n",
      " |              where `x` is a numpy array of image data\n",
      " |              (in the case of a single image input) or a list\n",
      " |              of numpy arrays (in the case with\n",
      " |              additional inputs) and `y` is a numpy array\n",
      " |              of corresponding labels. If 'sample_weight' is not None,\n",
      " |              the yielded tuples are of the form `(x, y, sample_weight)`.\n",
      " |              If `y` is None, only the numpy array `x` is returned.\n",
      " |  \n",
      " |  flow_from_dataframe(self, dataframe, directory=None, x_col='filename', y_col='class', weight_col=None, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', subset=None, interpolation='nearest', validate_filenames=True, **kwargs)\n",
      " |      Takes the dataframe and the path to a directory\n",
      " |       and generates batches of augmented/normalized data.\n",
      " |      \n",
      " |      **A simple tutorial can be found **[here](\n",
      " |                                  http://bit.ly/keras_flow_from_dataframe).\n",
      " |      \n",
      " |      # Arguments\n",
      " |          dataframe: Pandas dataframe containing the filepaths relative to\n",
      " |              `directory` (or absolute paths if `directory` is None) of the\n",
      " |              images in a string column. It should include other column/s\n",
      " |              depending on the `class_mode`:\n",
      " |              - if `class_mode` is `\"categorical\"` (default value) it must\n",
      " |                  include the `y_col` column with the class/es of each image.\n",
      " |                  Values in column can be string/list/tuple if a single class\n",
      " |                  or list/tuple if multiple classes.\n",
      " |              - if `class_mode` is `\"binary\"` or `\"sparse\"` it must include\n",
      " |                  the given `y_col` column with class values as strings.\n",
      " |              - if `class_mode` is `\"raw\"` or `\"multi_output\"` it should contain\n",
      " |              the columns specified in `y_col`.\n",
      " |              - if `class_mode` is `\"input\"` or `None` no extra column is needed.\n",
      " |          directory: string, path to the directory to read images from. If `None`,\n",
      " |              data in `x_col` column should be absolute paths.\n",
      " |          x_col: string, column in `dataframe` that contains the filenames (or\n",
      " |              absolute paths if `directory` is `None`).\n",
      " |          y_col: string or list, column/s in `dataframe` that has the target data.\n",
      " |          weight_col: string, column in `dataframe` that contains the sample\n",
      " |              weights. Default: `None`.\n",
      " |          target_size: tuple of integers `(height, width)`, default: `(256, 256)`.\n",
      " |              The dimensions to which all images found will be resized.\n",
      " |          color_mode: one of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
      " |              Whether the images will be converted to have 1 or 3 color channels.\n",
      " |          classes: optional list of classes (e.g. `['dogs', 'cats']`).\n",
      " |              Default: None. If not provided, the list of classes will be\n",
      " |              automatically inferred from the `y_col`,\n",
      " |              which will map to the label indices, will be alphanumeric).\n",
      " |              The dictionary containing the mapping from class names to class\n",
      " |              indices can be obtained via the attribute `class_indices`.\n",
      " |          class_mode: one of \"binary\", \"categorical\", \"input\", \"multi_output\",\n",
      " |              \"raw\", sparse\" or None. Default: \"categorical\".\n",
      " |              Mode for yielding the targets:\n",
      " |              - `\"binary\"`: 1D numpy array of binary labels,\n",
      " |              - `\"categorical\"`: 2D numpy array of one-hot encoded labels.\n",
      " |                  Supports multi-label output.\n",
      " |              - `\"input\"`: images identical to input images (mainly used to\n",
      " |                  work with autoencoders),\n",
      " |              - `\"multi_output\"`: list with the values of the different columns,\n",
      " |              - `\"raw\"`: numpy array of values in `y_col` column(s),\n",
      " |              - `\"sparse\"`: 1D numpy array of integer labels,\n",
      " |              - `None`, no targets are returned (the generator will only yield\n",
      " |                  batches of image data, which is useful to use in\n",
      " |                  `model.predict_generator()`).\n",
      " |          batch_size: size of the batches of data (default: 32).\n",
      " |          shuffle: whether to shuffle the data (default: True)\n",
      " |          seed: optional random seed for shuffling and transformations.\n",
      " |          save_to_dir: None or str (default: None).\n",
      " |              This allows you to optionally specify a directory\n",
      " |              to which to save the augmented pictures being generated\n",
      " |              (useful for visualizing what you are doing).\n",
      " |          save_prefix: str. Prefix to use for filenames of saved pictures\n",
      " |              (only relevant if `save_to_dir` is set).\n",
      " |          save_format: one of \"png\", \"jpeg\"\n",
      " |              (only relevant if `save_to_dir` is set). Default: \"png\".\n",
      " |          follow_links: whether to follow symlinks inside class subdirectories\n",
      " |              (default: False).\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |              `validation_split` is set in `ImageDataGenerator`.\n",
      " |          interpolation: Interpolation method used to resample the image if the\n",
      " |              target size is different from that of the loaded image.\n",
      " |              Supported methods are `\"nearest\"`, `\"bilinear\"`, and `\"bicubic\"`.\n",
      " |              If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
      " |              supported. If PIL version 3.4.0 or newer is installed, `\"box\"` and\n",
      " |              `\"hamming\"` are also supported. By default, `\"nearest\"` is used.\n",
      " |          validate_filenames: Boolean, whether to validate image filenames in\n",
      " |              `x_col`. If `True`, invalid images will be ignored. Disabling this\n",
      " |              option can lead to speed-up in the execution of this function.\n",
      " |              Default: `True`.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `DataFrameIterator` yielding tuples of `(x, y)`\n",
      " |          where `x` is a numpy array containing a batch\n",
      " |          of images with shape `(batch_size, *target_size, channels)`\n",
      " |          and `y` is a numpy array of corresponding labels.\n",
      " |  \n",
      " |  flow_from_directory(self, directory, target_size=(256, 256), color_mode='rgb', classes=None, class_mode='categorical', batch_size=32, shuffle=True, seed=None, save_to_dir=None, save_prefix='', save_format='png', follow_links=False, subset=None, interpolation='nearest')\n",
      " |      Takes the path to a directory & generates batches of augmented data.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          directory: string, path to the target directory.\n",
      " |              It should contain one subdirectory per class.\n",
      " |              Any PNG, JPG, BMP, PPM or TIF images\n",
      " |              inside each of the subdirectories directory tree\n",
      " |              will be included in the generator.\n",
      " |              See [this script](\n",
      " |              https://gist.github.com/fchollet/0830affa1f7f19fd47b06d4cf89ed44d)\n",
      " |              for more details.\n",
      " |          target_size: Tuple of integers `(height, width)`,\n",
      " |              default: `(256, 256)`.\n",
      " |              The dimensions to which all images found will be resized.\n",
      " |          color_mode: One of \"grayscale\", \"rgb\", \"rgba\". Default: \"rgb\".\n",
      " |              Whether the images will be converted to\n",
      " |              have 1, 3, or 4 channels.\n",
      " |          classes: Optional list of class subdirectories\n",
      " |              (e.g. `['dogs', 'cats']`). Default: None.\n",
      " |              If not provided, the list of classes will be automatically\n",
      " |              inferred from the subdirectory names/structure\n",
      " |              under `directory`, where each subdirectory will\n",
      " |              be treated as a different class\n",
      " |              (and the order of the classes, which will map to the label\n",
      " |              indices, will be alphanumeric).\n",
      " |              The dictionary containing the mapping from class names to class\n",
      " |              indices can be obtained via the attribute `class_indices`.\n",
      " |          class_mode: One of \"categorical\", \"binary\", \"sparse\",\n",
      " |              \"input\", or None. Default: \"categorical\".\n",
      " |              Determines the type of label arrays that are returned:\n",
      " |              - \"categorical\" will be 2D one-hot encoded labels,\n",
      " |              - \"binary\" will be 1D binary labels,\n",
      " |                  \"sparse\" will be 1D integer labels,\n",
      " |              - \"input\" will be images identical\n",
      " |                  to input images (mainly used to work with autoencoders).\n",
      " |              - If None, no labels are returned\n",
      " |                (the generator will only yield batches of image data,\n",
      " |                which is useful to use with `model.predict_generator()`).\n",
      " |                Please note that in case of class_mode None,\n",
      " |                the data still needs to reside in a subdirectory\n",
      " |                of `directory` for it to work correctly.\n",
      " |          batch_size: Size of the batches of data (default: 32).\n",
      " |          shuffle: Whether to shuffle the data (default: True)\n",
      " |              If set to False, sorts the data in alphanumeric order.\n",
      " |          seed: Optional random seed for shuffling and transformations.\n",
      " |          save_to_dir: None or str (default: None).\n",
      " |              This allows you to optionally specify\n",
      " |              a directory to which to save\n",
      " |              the augmented pictures being generated\n",
      " |              (useful for visualizing what you are doing).\n",
      " |          save_prefix: Str. Prefix to use for filenames of saved pictures\n",
      " |              (only relevant if `save_to_dir` is set).\n",
      " |          save_format: One of \"png\", \"jpeg\"\n",
      " |              (only relevant if `save_to_dir` is set). Default: \"png\".\n",
      " |          follow_links: Whether to follow symlinks inside\n",
      " |              class subdirectories (default: False).\n",
      " |          subset: Subset of data (`\"training\"` or `\"validation\"`) if\n",
      " |              `validation_split` is set in `ImageDataGenerator`.\n",
      " |          interpolation: Interpolation method used to\n",
      " |              resample the image if the\n",
      " |              target size is different from that of the loaded image.\n",
      " |              Supported methods are `\"nearest\"`, `\"bilinear\"`,\n",
      " |              and `\"bicubic\"`.\n",
      " |              If PIL version 1.1.3 or newer is installed, `\"lanczos\"` is also\n",
      " |              supported. If PIL version 3.4.0 or newer is installed,\n",
      " |              `\"box\"` and `\"hamming\"` are also supported.\n",
      " |              By default, `\"nearest\"` is used.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A `DirectoryIterator` yielding tuples of `(x, y)`\n",
      " |              where `x` is a numpy array containing a batch\n",
      " |              of images with shape `(batch_size, *target_size, channels)`\n",
      " |              and `y` is a numpy array of corresponding labels.\n",
      " |  \n",
      " |  get_random_transform(self, img_shape, seed=None)\n",
      " |      Generates random parameters for a transformation.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          seed: Random seed.\n",
      " |          img_shape: Tuple of integers.\n",
      " |              Shape of the image that is transformed.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A dictionary containing randomly chosen parameters describing the\n",
      " |          transformation.\n",
      " |  \n",
      " |  random_transform(self, x, seed=None)\n",
      " |      Applies a random transformation to an image.\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: 3D tensor, single image.\n",
      " |          seed: Random seed.\n",
      " |      \n",
      " |      # Returns\n",
      " |          A randomly transformed version of the input (same shape).\n",
      " |  \n",
      " |  standardize(self, x)\n",
      " |      Applies the normalization configuration in-place to a batch of inputs.\n",
      " |      \n",
      " |      `x` is changed in-place since the function is mainly used internally\n",
      " |      to standarize images and feed them to your network. If a copy of `x`\n",
      " |      would be created instead it would have a significant performance cost.\n",
      " |      If you want to apply this method without changing the input in-place\n",
      " |      you can call the method creating a copy before:\n",
      " |      \n",
      " |      standarize(np.copy(x))\n",
      " |      \n",
      " |      # Arguments\n",
      " |          x: Batch of inputs to be normalized.\n",
      " |      \n",
      " |      # Returns\n",
      " |          The inputs, normalized.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from keras_preprocessing.image.image_data_generator.ImageDataGenerator:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ImageDataGenerator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_gen =ImageDataGenerator(rotation_range=20,\n",
    "                             featurewise_center=True,\n",
    "                             featurewise_std_normalization=True,\n",
    "                             width_shift_range=0.2,\n",
    "                             height_shift_range=0.2,\n",
    "                             rescale=1/255,\n",
    "                             shear_range=0.2,\n",
    "                             zoom_range=0.2,\n",
    "                             horizontal_flip=False,\n",
    "                             vertical_flip=True,\n",
    "                             fill_mode='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1244 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x1a2e11c208>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set path to the image\n",
    "image_gen.flow_from_directory(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 265 images belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.image.directory_iterator.DirectoryIterator at 0x1a2e0589e8>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_gen.flow_from_directory(test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "\n",
    "#model.add(Conv2D(filters=32, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\n",
    "#model.add(MaxPool2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3),input_shape=image_shape,activation='relu'))\n",
    "model.add(MaxPool2D(pool_size=(2,2)))\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "model.add(Dense(128,activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Dense(1,activation='sigmoid'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "opt = SGD(lr=0.1)\n",
    "model.compile(loss = 'binary_crossentropy',\n",
    "             optimizer = opt,\n",
    "             metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_27 (Conv2D)           (None, 148, 198, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_27 (MaxPooling (None, 74, 99, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_28 (Conv2D)           (None, 72, 97, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_28 (MaxPooling (None, 36, 48, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_29 (Conv2D)           (None, 34, 46, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_29 (MaxPooling (None, 17, 23, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_30 (Conv2D)           (None, 15, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_30 (MaxPooling (None, 7, 10, 128)        0         \n",
      "_________________________________________________________________\n",
      "flatten_7 (Flatten)          (None, 8960)              0         \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 128)               1147008   \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 1)                 129       \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 1,277,313\n",
      "Trainable params: 1,277,313\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stop = EarlyStopping(monitor = 'val_loss', patience=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1244 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_image_gen = image_gen.flow_from_directory(train_path, \n",
    "                                               target_size=image_shape[:2],\n",
    "                                               color_mode='rgb',\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 265 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "test_image_gen = image_gen.flow_from_directory(test_path, \n",
    "                                               target_size=image_shape[:2],\n",
    "                                               color_mode='rgb',\n",
    "                                               batch_size=batch_size,\n",
    "                                               class_mode='binary',\n",
    "                                               shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cats': 0, 'dogs': 1}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_image_gen.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/axr524/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:716: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n",
      "/Users/axr524/anaconda3/lib/python3.7/site-packages/keras_preprocessing/image/image_data_generator.py:724: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.\n",
      "  warnings.warn('This ImageDataGenerator specifies '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78/78 [==============================] - 51s 649ms/step - loss: 0.6960 - accuracy: 0.5008 - val_loss: 0.6948 - val_accuracy: 0.4755\n",
      "Epoch 2/200\n",
      "78/78 [==============================] - 56s 716ms/step - loss: 0.6924 - accuracy: 0.5064 - val_loss: 0.6918 - val_accuracy: 0.5774\n",
      "Epoch 3/200\n",
      "78/78 [==============================] - 54s 690ms/step - loss: 0.6929 - accuracy: 0.5297 - val_loss: 0.6860 - val_accuracy: 0.6491\n",
      "Epoch 4/200\n",
      "25/78 [========>.....................] - ETA: 29s - loss: 0.6918 - accuracy: 0.5250"
     ]
    }
   ],
   "source": [
    "results = model.fit_generator(train_image_gen,epochs=200,validation_data=test_image_gen,callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
